# Попробуем искать параметры не прямой, а полиномиальной функции.

# Градиентный спуск и полиномиальная модель

# Теперь матрица X имеет другой вид, но её суть осталась той же —
# это матрица коэффициентов перед неизвестными.

# Решением такой системы является набор коэффициентов, который
# задаёт параболу.

# Аналогично случаю с прямой, мы можем искать решение в среднем —
# минимизируя функцию ошибки.

import numpy as np

# Исходные точки.
x = np.array([0., 1., 2., 3., 4., 5., 6., 7., 8., 0., 1., 2., 3., 4., 5., 6., 7.,
       8., 0., 1., 2., 3., 4., 5., 6., 7., 8., 0., 1., 2., 3., 4., 5., 6.,
       7., 8., 0., 1., 2., 3., 4., 5., 6., 7., 8., 0., 1., 2., 3., 4.])
y = np.array([ 9. ,  7.5,  7.5, 10. ,  5.5,  7.5,  8.5,  5. ,  5.5,  5. ,  7. ,
        8.5,  8.5,  5. ,  6.5,  8.5,  3.5,  3. ,  5.5,  6. ,  7. ,  7. ,
        5.5,  6. ,  8.5,  7. ,  6. ,  6. ,  8. , 10. ,  7. ,  6. ,  4.5,
        6.5,  4.5,  6.5,  7. ,  8.5,  6. ,  6.5,  9.5,  8. ,  8. ,  5. ,
        3.5,  6. ,  9. ,  6.5,  5.5,  8. ])

# Функция потерь и её градиент.
def f(X, y, w):
    return np.mean((X @ w - y)**2)

def grad(X, y, w):
    return 2 / len(X) * X.T @ (X @ w - y)

# Составляем матрицу X.
X = np.stack([
    x ** 2,
		x,
    np.ones(len(x))
], axis=1)

# Инициализируем веса модели.
w = np.array([1, 2, 3])

# Производим градиентный спуск.
gamma = 1e-3
max_iter = 10000
eps = 1e-5

# Делаем один шаг, чтобы проверить, что мы ещё не в минимуме.
f_old = f(X, y, w)
w = w - gamma * grad(X, y, w)
f_new = f(X, y, w)

# Текущий шаг.
i = 1

# Основной цикл.
while np.abs(f_new - f_old) > eps and i < max_iter:

	# Обновляем веса.
	w = w - gamma * grad(X, y, w)

	# Увеличиваем номер шага и обновляем значения функции.
	i = i + 1
	f_old = f_new
	f_new = f(X, y, w)

result = w
print(result)
print(f(X, y, w))

# В результате работы алгоритма Макс получил вектор весов
# [-0.11011743, 0.67058175, 6.44017367]. Получается, что из всех
# квадратичных функций лучше всех описывает данные Макса функция
# (мы указали коэффициенты с точностью до сотых)
# y=−0.11x^2+0.67x+6.44.

# По графику видно, что полиномиальная модель передаёт тренд с
# улучшением и последующим ухудшением самочувствия в течение дня.

# Такая модель называется полиномиальной регрессией. Этот подход
# также работает и с полиномами большей степени. Отличия будут
# только в количестве весов у модели и в добавлении столбцов
# в матрице X.

##### EXERCISE 3 #####

# Найдите параметры полиномиальной регрессии степени 3 по указанным
# данным с помощью градиентного спуска. Для этого создайте функцию
# для подсчёта ошибки f и функцию для подсчёта градиента grad.
# Подберите такие параметры, чтобы итоговое значение функции
# ошибки было ниже 0.48. Сохраните вектор найденных весов
# в переменной result и выведите на экран. Выведите финальное
# значение функции ошибки.

import numpy as np
import matplotlib.pyplot as plt

x = np.array([-9.93841085, -8.2398223 , -9.06505398, -7.35203062, -5.82847285,
        -5.08181713, -3.37174708, -3.6361873 , -0.06175255,  0.09106786,
         1.46721029,  0.41053496,  1.71012239,  1.84871104,  6.68526793,
         6.82543486,  6.64741998,  8.01775519,  8.57773967, 11.8291112 ])
y = np.array([ 0.99747243,  3.28729745,  2.0644648 ,  2.88068415, -0.05454181,
         0.63703982,  0.06238763,  0.25253028,  0.06582577,  0.05755049,
         0.20686123, -0.03885818,  0.40837474,  0.52833438,  0.25072492,
         0.26994154,  0.29157405,  0.52908138, -0.04000158, -0.98596774])

# Creating a matrix X - матрицу коэффициентов перед неизвестными
X = np.stack([x**3, x**2, x, np.ones(len(x))], axis=1)
# print(X)

# Рассчитываем коэффициенты прямой по формуле.
# w - вектор коэффициентов [k, m].
result = np.linalg.inv(X.T @ X) @ X.T @ y

# Функция потерь и её градиент.
def f(X, y, w):
    return np.mean((X @ w - y)**2)

def grad(X, y, w):
    return 2 / len(X) * X.T @ (X @ w - y)

print(result)
print(grad(X, y, result))
print(f(X, y, result))

# Код для построения графика.
plt.figure(figsize=(8, 6))
plt.scatter(x, y, s=100)
X = np.stack([
   np.linspace(x.min(), x.max(), 30),
   np.linspace(x.min(), x.max(), 30) ** 2,
   np.linspace(x.min(), x.max(), 30) ** 3,
   np.ones(30)
], axis=1)
plt.plot(X[:, 0], X @ result, c="C1", linewidth=2)
plt.show()