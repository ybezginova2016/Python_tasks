# Алгоритм стохастического градиентного спуска работает так:

# На вход поступают гиперпараметры: размер батча, количество эпох
# и величина шага.
# Определяются начальные значения весов модели.
# Для каждой эпохи обучающая выборка разбивается на батчи.

# Для каждого батча:
# 4.1. Вычисляется градиент функции потерь;
# 4.2. Обновляются веса модели (к текущим значениям весов
# прибавляется антиградиент, умноженный на величину шага).
# Алгоритм возвращает последние веса модели.

# Найдём вычислительную сложность SGD. Обозначим:
# n — количество объектов во всей обучающей выборке;
# b — размер батча;
# p — количество признаков.

# вычислительная сложность выполнения шага для одного батча =
# T(n, b, p) ~ bp => Равна вычислительной сложности шага на
# всей выборке. Только объектов намного меньше.

# Чему равна вычислительная сложность выполнения одной эпохи,
# если выборка на батчи делится нацело, то есть n/b — целое число.
# T(n, b, p) ~ np
# В одной эпохе n/b итераций, сложность каждой — bp. Решение такое:
# n/b * bp = np.

# Если набор данных большой и эпох мало, сложность обучения такая:
# T(n, b, p) ~ np. Алгоритмом SGD линейная регрессия обучается быстрее,
# чем прямым методом.